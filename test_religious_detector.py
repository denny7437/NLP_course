#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è —á—É–≤—Å—Ç–≤ –≤–µ—Ä—É—é—â–∏—Ö
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
import os

class ReligiousContentTester:
    def __init__(self, model_path="./religious_detector_manual"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–µ—Ä–∞
        
        Args:
            model_path: –ø—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å)
        """
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self.device = torch.device('cpu')  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    
    def initialize_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏–∑ {self.model_path}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        if os.path.exists(self.model_path):
            print("‚úÖ –ù–∞–π–¥–µ–Ω–∞ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å!")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
                print("üéØ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –û–ë–£–ß–ï–ù–ù–ê–Ø –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–µ–ª–∏–≥–∏–æ–∑–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
                print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
                self._load_base_model()
        else:
            print("‚ö†Ô∏è –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∑–∞–≥—Ä—É–∂–∞—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
            self._load_base_model()
        
        self.model.to(self.device)
        self.model.eval()
        print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
    
    def _load_base_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ ModernBERT"""
        model_name = "answerdotai/ModernBERT-base"
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, 
            num_labels=2,
            problem_type="single_label_classification"
        )
        print("ü§ñ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –±–∞–∑–æ–≤–∞—è ModernBERT (–Ω–µ–æ–±—É—á–µ–Ω–Ω–∞—è)")
    
    def predict(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            tuple: (predictions, probabilities)
        """
        if self.model is None or self.tokenizer is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ initialize_model()")
        
        predictions = []
        probabilities = []
        
        with torch.no_grad():
            for text in texts:
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                inputs = self.tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    padding=True,
                    max_length=512
                ).to(self.device)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                outputs = self.model(**inputs)
                logits = outputs.logits
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                probs = torch.softmax(logits, dim=-1)
                probs_np = probs.cpu().numpy()[0]
                
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å
                predicted_class = torch.argmax(logits, dim=-1).item()
                
                predictions.append(predicted_class)
                probabilities.append(probs_np)
        
        return predictions, probabilities
    
    def test_examples(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö"""
        print("\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ü–†–ò–ú–ï–†–ê–•")
        print("="*50)
        
        # –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_cases = [
            ("–†–µ–ª–∏–≥–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –ª—é–¥—è–º –Ω–∞–π—Ç–∏ —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏", 0),  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π
            ("–í—Å–µ –≤–µ—Ä—É—é—â–∏–µ - –¥—É—Ä–∞–∫–∏ –∏ —Ñ–∞–Ω–∞—Ç–∏–∫–∏", 1),  # –û–ø–∞—Å–Ω—ã–π
            ("–î—É—Ö–æ–≤–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º–∏", 0),  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π
            ("–¶–µ—Ä–∫–≤–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–∂–∂–µ–Ω—ã –¥–æ—Ç–ª–∞", 1),  # –û–ø–∞—Å–Ω—ã–π
            ("–ö–∞–∂–¥—ã–π –∏–º–µ–µ—Ç –ø—Ä–∞–≤–æ –Ω–∞ —Å–≤–æ—é –≤–µ—Ä—É", 0),  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π
            ("–†–µ–ª–∏–≥–∏—è - —ç—Ç–æ —è–¥ –¥–ª—è –æ–±—â–µ—Å—Ç–≤–∞", 1),  # –û–ø–∞—Å–Ω—ã–π
        ]
        
        correct = 0
        total = len(test_cases)
        
        for i, (text, expected) in enumerate(test_cases, 1):
            predictions, probabilities = self.predict([text])
            predicted = predictions[0]
            prob = probabilities[0]
            
            print(f"\n{i}. –¢–µ–∫—Å—Ç: {text}")
            print(f"   –û–∂–∏–¥–∞–µ–º—ã–π –∫–ª–∞—Å—Å: {expected} ({'–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π' if expected == 0 else '–û–ø–∞—Å–Ω—ã–π'})")
            print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π: {predicted} ({'–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π' if predicted == 0 else '–û–ø–∞—Å–Ω—ã–π'})")
            print(f"   –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π={prob[0]:.3f}, –û–ø–∞—Å–Ω—ã–π={prob[1]:.3f}")
            
            if predicted == expected:
                print("   ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ!")
                correct += 1
            else:
                print("   ‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ!")
        
        accuracy = correct / total
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {correct}/{total}")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%}")
        
        if accuracy >= 0.8:
            print("   üéâ –û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –º–æ–¥–µ–ª–∏!")
        elif accuracy >= 0.6:
            print("   üëç –ù–µ–ø–ª–æ—Ö–∞—è —Ä–∞–±–æ—Ç–∞ –º–æ–¥–µ–ª–∏")
        else:
            print("   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏—è")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    tester = ReligiousContentTester()
    tester.initialize_model()
    tester.test_examples()

if __name__ == "__main__":
    main()