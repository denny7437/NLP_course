# –ú–æ–¥–µ–ª–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–µ–ª–∏–≥–∏–æ–∑–Ω–æ–≥–æ hate speech

–ò–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π GitHub (100MB –Ω–∞ —Ñ–∞–π–ª), –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π. –û–¥–Ω–∞–∫–æ –≤—ã –º–æ–∂–µ—Ç–µ –ª–µ–≥–∫–æ –æ–±—É—á–∏—Ç—å –∏—Ö –∑–∞–Ω–æ–≤–æ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏.

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–µ–π

```
models/
‚îú‚îÄ‚îÄ bert_religious_classifier/           # –ë–∞–∑–æ–≤–∞—è BERT –º–æ–¥–µ–ª—å (57.5% —Ç–æ—á–Ω–æ—Å—Ç—å)
‚îú‚îÄ‚îÄ optimized_religious_classifier/      # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (99.0% —Ç–æ—á–Ω–æ—Å—Ç—å)
‚îî‚îÄ‚îÄ bert_religious_classifier_improved/  # –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (98.6% —Ç–æ—á–Ω–æ—Å—Ç—å)
```

## üöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

### 1. –ë–∞–∑–æ–≤–∞—è BERT –º–æ–¥–µ–ª—å
```bash
python scripts/train_bert.py
```

### 2. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
```bash
python train_final_model.py
```

### 3. –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
```bash
python train_bert_detector.py
```

## üìä –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–µ–π

| –ú–æ–¥–µ–ª—å | –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | –¢–æ—á–Ω–æ—Å—Ç—å (train) | –¢–æ—á–Ω–æ—Å—Ç—å (test) | –†–∞–∑–º–µ—Ä |
|--------|-------------|-----------|------------------|-----------------|--------|
| **BERT v1** | `cointegrated/rubert-tiny2` | 29M | 57.5% | 57.5% | ~111MB |
| **BERT v2** | `cointegrated/rubert-tiny2` | 29M | 99.0% | 65.6% | ~111MB |
| **BERT v3** | `DeepPavlov/rubert-base-cased` | 180M | 98.6% | 70.6% | ~678MB |

## üîß –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

- **GPU**: NVIDIA A100 80GB (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è) –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è
- **RAM**: 16GB+ —Å–∏—Å—Ç–µ–º–Ω–æ–π –ø–∞–º—è—Ç–∏
- **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è**: 
  - BERT v1/v2: ~10-15 –º–∏–Ω—É—Ç
  - BERT v3: ~30 —Å–µ–∫—É–Ω–¥ (—Å GPU)
- **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**: —Å–º. `requirements.txt`

## üíæ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–∫–∞—Ö:

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# –ó–∞–≥—Ä—É–∑–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
tokenizer = AutoTokenizer.from_pretrained('./bert_religious_classifier_improved')
model = AutoModelForSequenceClassification.from_pretrained('./bert_religious_classifier_improved')
```

## üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
import torch

def predict(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
    with torch.no_grad():
        outputs = model(**inputs)
        prediction = torch.argmax(outputs.logits, dim=-1)
    return "–û–ø–∞—Å–Ω—ã–π" if prediction.item() == 1 else "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π"

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
result = predict("–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏", model, tokenizer)
print(result)
```

## üìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Rule-based

–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å rule-based –ø–æ–¥—Ö–æ–¥–æ–º –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:

```bash
python compare_bert_vs_rules.py
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: Rule-based –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç 100% —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏—Ö —Ä–µ—Å—É—Ä—Å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö.

## üîÑ –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

1. –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ: `python compare_bert_vs_rules.py`
3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –≤ README.md

## üìù –ü—Ä–∏–º–µ—á–∞–Ω–∏—è

- –ú–æ–¥–µ–ª–∏ –æ–±—É—á–∞–ª–∏—Å—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏–∑ 1,800 –ø—Ä–∏–º–µ—Ä–æ–≤
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å –Ω–∞ 160 –ø—Ä–∏–º–µ—Ä–∞—Ö —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π
- –í—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ seed=42 