#!/usr/bin/env python3
"""
–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
import os

class SafeReligiousDetector:
    def __init__(self, model_path="./religious_detector_manual"):
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self.device = torch.device('cpu')
        
    def load_model(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏–∑ {self.model_path}...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = AutoConfig.from_pretrained(self.model_path)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        self.model = AutoModelForSequenceClassification.from_config(config)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
        state_dict_path = os.path.join(self.model_path, "pytorch_model.bin")
        if os.path.exists(state_dict_path):
            state_dict = torch.load(state_dict_path, map_location=self.device, weights_only=True)
            self.model.load_state_dict(state_dict)
            print("‚úÖ –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        else:
            print("‚ö†Ô∏è –§–∞–π–ª –≤–µ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
            
        self.model.to(self.device)
        self.model.eval()
        print("üéØ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    
    def predict(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if self.model is None or self.tokenizer is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ load_model()")
            
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=256
        ).to(self.device)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            prediction = torch.argmax(logits, dim=-1).item()
        
        return prediction, probs[0].cpu().numpy()
    
    def batch_predict(self, texts):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        predictions = []
        probabilities = []
        
        for text in texts:
            pred, prob = self.predict(text)
            predictions.append(pred)
            probabilities.append(prob)
            
        return predictions, probabilities

def test_trained_model():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò")
    print("="*50)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
    detector = SafeReligiousDetector()
    detector.load_model()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    test_cases = [
        ("–†–µ–ª–∏–≥–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –ª—é–¥—è–º –Ω–∞–π—Ç–∏ —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏", 0),  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π
        ("–í—Å–µ –≤–µ—Ä—É—é—â–∏–µ - –¥—É—Ä–∞–∫–∏ –∏ —Ñ–∞–Ω–∞—Ç–∏–∫–∏", 1),  # –û–ø–∞—Å–Ω—ã–π
        ("–î—É—Ö–æ–≤–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º–∏", 0),  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π
        ("–¶–µ—Ä–∫–≤–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–∂–∂–µ–Ω—ã –¥–æ—Ç–ª–∞", 1),  # –û–ø–∞—Å–Ω—ã–π
        ("–ö–∞–∂–¥—ã–π –∏–º–µ–µ—Ç –ø—Ä–∞–≤–æ –Ω–∞ —Å–≤–æ—é –≤–µ—Ä—É", 0),  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π
        ("–†–µ–ª–∏–≥–∏—è - —ç—Ç–æ —è–¥ –¥–ª—è –æ–±—â–µ—Å—Ç–≤–∞", 1),  # –û–ø–∞—Å–Ω—ã–π
        ("–ú–æ–ª–∏—Ç–≤–∞ –ø—Ä–∏–Ω–æ—Å–∏—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –ø–æ–∫–æ–π", 0),  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π
        ("–†–µ–ª–∏–≥–∏–æ–∑–Ω—ã–µ –ª–∏–¥–µ—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–Ω–∏—á—Ç–æ–∂–µ–Ω—ã", 1),  # –û–ø–∞—Å–Ω—ã–π
    ]
    
    correct = 0
    total = len(test_cases)
    
    print("\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    
    for i, (text, expected) in enumerate(test_cases, 1):
        pred, prob = detector.predict(text)
        
        safe_prob = prob[0] * 100
        dangerous_prob = prob[1] * 100
        confidence = max(prob) * 100
        
        print(f"\n{i}. {text}")
        print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: {'–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π' if expected == 0 else '–û–ø–∞—Å–Ω—ã–π'}")
        print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {'–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π' if pred == 0 else '–û–ø–∞—Å–Ω—ã–π'}")
        print(f"   –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π={safe_prob:.1f}%, –û–ø–∞—Å–Ω—ã–π={dangerous_prob:.1f}%")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1f}%")
        
        if pred == expected:
            print("   ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ!")
            correct += 1
        else:
            print("   ‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ!")
    
    accuracy = correct / total
    print(f"\nüìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {correct}/{total}")
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%}")
    
    if accuracy >= 0.9:
        print("   üèÜ –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –º–æ–¥–µ–ª–∏!")
    elif accuracy >= 0.8:
        print("   üéâ –û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –º–æ–¥–µ–ª–∏!")
    elif accuracy >= 0.7:
        print("   üëç –•–æ—Ä–æ—à–∞—è —Ä–∞–±–æ—Ç–∞ –º–æ–¥–µ–ª–∏!")
    else:
        print("   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")

if __name__ == "__main__":
    test_trained_model() 