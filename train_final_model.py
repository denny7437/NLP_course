#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ BERT –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è —á—É–≤—Å—Ç–≤ –≤–µ—Ä—É—é—â–∏—Ö
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ª—É—á—à–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
"""

import pandas as pd
import numpy as np
import torch
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding,
    EarlyStoppingCallback
)
import os
from torch.utils.data import Dataset
import logging
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class OptimizedReligiousDataset(Dataset):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ä–µ–ª–∏–≥–∏–æ–∑–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }


def analyze_dataset(df):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    logger.info("üìä –ê–ù–ê–õ–ò–ó –î–ê–¢–ê–°–ï–¢–ê")
    logger.info("=" * 50)
    
    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_samples = len(df)
    logger.info(f"üìà –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total_samples}")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    class_counts = df['label'].value_counts().sort_index()
    logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    for label, count in class_counts.items():
        label_name = "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π" if label == 0 else "–û–ø–∞—Å–Ω—ã–π"
        percentage = (count / total_samples) * 100
        logger.info(f"   {label_name} (label={label}): {count} ({percentage:.1f}%)")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
    text_lengths = df['text'].str.len()
    word_counts = df['text'].str.split().str.len()
    
    logger.info(f"\nüìù –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤:")
    logger.info(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–∏–º–≤–æ–ª–æ–≤: {text_lengths.mean():.1f}")
    logger.info(f"   –ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–∏–º–≤–æ–ª–æ–≤: {text_lengths.median():.1f}")
    logger.info(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {text_lengths.max()}")
    logger.info(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {word_counts.mean():.1f}")
    logger.info(f"   –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {word_counts.median():.1f}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    balance_ratio = min(class_counts) / max(class_counts)
    logger.info(f"\n‚öñÔ∏è –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {balance_ratio:.3f}")
    if balance_ratio < 0.5:
        logger.warning("‚ö†Ô∏è –î–∞—Ç–∞—Å–µ—Ç –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å")
    
    return {
        'total_samples': total_samples,
        'class_distribution': class_counts,
        'balance_ratio': balance_ratio,
        'avg_length': text_lengths.mean(),
        'avg_words': word_counts.mean()
    }


class OptimizedTrainer(Trainer):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """–í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get('logits')
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è
        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.5], device=labels.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss


def compute_metrics(eval_pred):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(labels, predictions)
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    report = classification_report(
        labels, predictions,
        target_names=['–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π', '–û–ø–∞—Å–Ω—ã–π'],
        output_dict=True
    )
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(labels, predictions)
    
    return {
        'accuracy': accuracy,
        'f1_safe': report['–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π']['f1-score'],
        'f1_dangerous': report['–û–ø–∞—Å–Ω—ã–π']['f1-score'],
        'precision_safe': report['–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π']['precision'],
        'precision_dangerous': report['–û–ø–∞—Å–Ω—ã–π']['precision'],
        'recall_safe': report['–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π']['recall'],
        'recall_dangerous': report['–û–ø–∞—Å–Ω—ã–π']['recall'],
        'macro_f1': report['macro avg']['f1-score']
    }


def train_optimized_model():
    """–û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    logger.info("üöÄ –ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø BERT")
    logger.info("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("üìÇ –ó–∞–≥—Ä—É–∂–∞—é —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
    df = pd.read_csv("train - –õ–∏—Å—Ç1.csv")
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_stats = analyze_dataset(df)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    texts = df['text'].tolist()
    labels = df['label'].tolist()
    
    # –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    model_name = "cointegrated/rubert-tiny2"  # –ë—ã—Å—Ç—Ä–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä—É—Å—Å–∫–∞—è BERT
    logger.info(f"ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å: {model_name}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
    logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
        problem_type="single_label_classification"
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    max_length = min(256, int(dataset_stats['avg_length'] * 1.5))  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
    logger.info(f"üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_length}")
    
    train_dataset = OptimizedReligiousDataset(texts, labels, tokenizer, max_length)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–∞—Ç–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    training_args = TrainingArguments(
        output_dir='./optimized_religious_classifier',
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        num_train_epochs=5,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        per_device_train_batch_size=16,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        per_device_eval_batch_size=32,
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        learning_rate=3e-5,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π learning rate –¥–ª—è BERT
        weight_decay=0.01,
        warmup_steps=200,  # –†–∞–∑–æ–≥—Ä–µ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        logging_steps=50,
        save_steps=200,
        save_strategy="steps",
        save_total_limit=3,
        
        # –û—Ü–µ–Ω–∫–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
        eval_strategy="no",  # –û—Ç–∫–ª—é—á–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        dataloader_num_workers=4,
        remove_unused_columns=False,
        
        # –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        seed=42,
        fp16=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ CPU
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        push_to_hub=False,
        report_to=None,  # –û—Ç–∫–ª—é—á–∞–µ–º wandb/tensorboard
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        gradient_accumulation_steps=2,
        max_grad_norm=1.0,
        
        # –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ learning rate
        lr_scheduler_type="cosine",
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU
        use_cpu=True,
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = OptimizedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    logger.info("üéØ –ù–ê–ß–ò–ù–ê–Æ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï...")
    logger.info(f"   üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_dataset)}")
    logger.info(f"   üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {training_args.num_train_epochs}")
    logger.info(f"   üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {training_args.per_device_train_batch_size}")
    logger.info(f"   üìà Learning rate: {training_args.learning_rate}")
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    train_result = trainer.train()
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    logger.info("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å: {train_result.training_loss:.4f}")
    logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {train_result.metrics['train_runtime']:.2f} —Å–µ–∫—É–Ω–¥")
    logger.info(f"üîÑ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤: {train_result.metrics['train_steps_per_second']:.2f} —à–∞–≥–æ–≤/—Å–µ–∫")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("üíæ –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
    trainer.save_model()
    tokenizer.save_pretrained('./optimized_religious_classifier')
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    logger.info("üß™ –¢–µ—Å—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤—Å–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    train_predictions = trainer.predict(train_dataset)
    predicted_labels = np.argmax(train_predictions.predictions, axis=1)
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(labels, predicted_labels)
    report = classification_report(
        labels, predicted_labels,
        target_names=['–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π', '–û–ø–∞—Å–Ω—ã–π'],
        output_dict=True
    )
    cm = confusion_matrix(labels, predicted_labels)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("\nüéØ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –¢–†–ï–ù–ò–†–û–í–û–ß–ù–´–• –î–ê–ù–ù–´–•:")
    logger.info("=" * 55)
    logger.info(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    logger.info(f"\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    logger.info(f"   –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç:")
    logger.info(f"     Precision: {report['–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π']['precision']:.4f}")
    logger.info(f"     Recall: {report['–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π']['recall']:.4f}")
    logger.info(f"     F1-score: {report['–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π']['f1-score']:.4f}")
    
    logger.info(f"   –û–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç:")
    logger.info(f"     Precision: {report['–û–ø–∞—Å–Ω—ã–π']['precision']:.4f}")
    logger.info(f"     Recall: {report['–û–ø–∞—Å–Ω—ã–π']['recall']:.4f}")
    logger.info(f"     F1-score: {report['–û–ø–∞—Å–Ω—ã–π']['f1-score']:.4f}")
    
    logger.info(f"\nüìà –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
    logger.info(f"   Macro F1-score: {report['macro avg']['f1-score']:.4f}")
    logger.info(f"   Weighted F1-score: {report['weighted avg']['f1-score']:.4f}")
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    logger.info(f"\nüîç –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
    logger.info(f"                –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ")
    logger.info(f"            –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π  –û–ø–∞—Å–Ω—ã–π")
    logger.info(f"–ò—Å—Ç–∏–Ω–Ω–æ")
    logger.info(f"–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π      {cm[0,0]:4d}     {cm[0,1]:4d}")
    logger.info(f"–û–ø–∞—Å–Ω—ã–π         {cm[1,0]:4d}     {cm[1,1]:4d}")
    
    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    false_positives = cm[0,1]  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–∫ –æ–ø–∞—Å–Ω—ã–µ
    false_negatives = cm[1,0]  # –û–ø–∞—Å–Ω—ã–µ, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–∫ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ
    
    logger.info(f"\n‚ùå –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫:")
    logger.info(f"   –õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è (FP): {false_positives}")
    logger.info(f"   –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –æ–ø–∞—Å–Ω—ã–µ (FN): {false_negatives}")
    
    if false_positives > false_negatives:
        logger.info("   üìù –ú–æ–¥–µ–ª—å —Å–∫–ª–æ–Ω–Ω–∞ –∫ –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Å–ª–∏—à–∫–æ–º –æ—Å—Ç–æ—Ä–æ–∂–Ω–∞)")
    elif false_negatives > false_positives:
        logger.info("   üìù –ú–æ–¥–µ–ª—å –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –æ–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞)")
    else:
        logger.info("   üìù –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
    logger.info("\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ù–û–í–´–• –ü–†–ò–ú–ï–†–ê–•:")
    logger.info("=" * 45)
    
    test_examples = [
        ("–ú–æ–ª–∏—Ç–≤–∞ –ø–æ–º–æ–≥–∞–µ—Ç –Ω–∞–π—Ç–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –ø–æ–∫–æ–π –∏ –≥–∞—Ä–º–æ–Ω–∏—é", 0),
        ("–†–µ–ª–∏–≥–∏–æ–∑–Ω—ã–µ —Ç—Ä–∞–¥–∏—Ü–∏–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –ª—é–¥–µ–π –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞", 0),
        ("–î—É—Ö–æ–≤–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—é—Ç –ª–∏—á–Ω–æ—Å—Ç–Ω–æ–º—É —Ä–æ—Å—Ç—É", 0),
        ("–í—Å–µ –≤–µ—Ä—É—é—â–∏–µ - –¥—É—Ä–∞–∫–∏ –∏ —Ñ–∞–Ω–∞—Ç–∏–∫–∏ –±–µ–∑ –º–æ–∑–≥–æ–≤", 1),
        ("–¶–µ—Ä–∫–æ–≤—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —É–Ω–∏—á—Ç–æ–∂–µ–Ω–∞ –æ–≥–Ω–µ–º –∏ –º–µ—á–æ–º", 1),
        ("–†–µ–ª–∏–≥–∏—è - —ç—Ç–æ –æ–ø–∏—É–º –¥–ª—è –Ω–∞—Ä–æ–¥–∞ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —É–≥–Ω–µ—Ç–µ–Ω–∏—è", 1),
        ("–í–µ—Ä–∞ –≤ –±–æ–≥–∞ - –ø—Ä–∏–∑–Ω–∞–∫ –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–∞", 1),
        ("–†–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏ –∏–º–µ—é—Ç –ø—Ä–∞–≤–æ –Ω–∞ –º–∏—Ä–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ", 0),
    ]
    
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    model.eval()
    device = torch.device('cpu')
    model.to(device)
    
    correct_predictions = 0
    for i, (text, expected) in enumerate(test_examples, 1):
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding='max_length',
            max_length=max_length
        )
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            inputs = {k: v.to(device) for k, v in inputs.items()}
            outputs = model(**inputs)
            predicted_class = torch.argmax(outputs.logits, dim=-1).item()
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        expected_name = "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π" if expected == 0 else "–û–ø–∞—Å–Ω—ã–π"
        predicted_name = "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π" if predicted_class == 0 else "–û–ø–∞—Å–Ω—ã–π"
        correct = "‚úÖ" if predicted_class == expected else "‚ùå"
        
        if predicted_class == expected:
            correct_predictions += 1
        
        logger.info(f"  {i}. {correct} '{text[:60]}{'...' if len(text) > 60 else ''}'")
        logger.info(f"     –û–∂–∏–¥–∞–ª–æ—Å—å: {expected_name}, –ü–æ–ª—É—á–µ–Ω–æ: {predicted_name}")
    
    test_accuracy = correct_predictions / len(test_examples)
    logger.info(f"\nüìä –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö: {test_accuracy:.2%} ({correct_predictions}/{len(test_examples)})")
    
    # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    logger.info("\nüèÜ –ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò:")
    logger.info("=" * 35)
    
    if accuracy >= 0.95:
        logger.info("üåü –û–¢–õ–ò–ß–ù–û! –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    elif accuracy >= 0.90:
        logger.info("üéØ –•–û–†–û–®–û! –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    elif accuracy >= 0.80:
        logger.info("üìà –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û. –ï—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è")
    else:
        logger.info("‚ö†Ô∏è –¢–†–ï–ë–£–ï–¢ –î–û–†–ê–ë–û–¢–ö–ò. –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    logger.info(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    if false_positives > 50:
        logger.info("   - –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —Å–Ω–∏–∂–µ–Ω–∏–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏")
        logger.info("   - –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    if false_negatives > 20:
        logger.info("   - –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–∞—Å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏")
        logger.info("   - –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–∞ '–æ–ø–∞—Å–Ω—ã–π'")
    
    if accuracy < 0.90:
        logger.info("   - –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è")
        logger.info("   - –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏")
        logger.info("   - –£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö")
    
    logger.info("\nüéâ –û–ë–£–ß–ï–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –ú–û–î–ï–õ–ò –ó–ê–í–ï–†–®–ï–ù–û!")
    logger.info("‚ú® –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ ./optimized_religious_classifier")
    
    return {
        'accuracy': accuracy,
        'model_path': './optimized_religious_classifier',
        'training_loss': train_result.training_loss,
        'metrics': report
    }


if __name__ == "__main__":
    try:
        results = train_optimized_model()
        logger.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {results['accuracy']:.4f}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        raise 