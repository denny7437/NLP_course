#!/usr/bin/env python3
"""
–†—É—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
"""

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
import torch.nn.functional as F
import os

class SimpleDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

def train_and_save_manually():
    print("üî• –†—É—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ModernBERT")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv("train_diff - –õ–∏—Å—Ç1.csv")
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    safe_count = (df['label'] == 0).sum()
    dangerous_count = (df['label'] == 1).sum()
    print(f"‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω—ã—Ö: {safe_count} ({safe_count/len(df)*100:.1f}%)")
    print(f"üö® –û–ø–∞—Å–Ω—ã—Ö: {dangerous_count} ({dangerous_count/len(df)*100:.1f}%)")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_val, y_train, y_val = train_test_split(
        df['text'].values, df['label'].values, 
        test_size=0.2, random_state=42, stratify=df['label']
    )
    
    print(f"üîÑ –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö: {len(X_train)}, –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö: {len(X_val)}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model_name = "answerdotai/ModernBERT-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, num_labels=2
    )
    
    device = torch.device('cpu')
    model.to(device)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = SimpleDataset(X_train, y_train, tokenizer)
    val_dataset = SimpleDataset(X_val, y_val, tokenizer)
    
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    
    # –û–±—É—á–µ–Ω–∏–µ
    model.train()
    num_epochs = 2
    
    print("üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
    
    for epoch in range(num_epochs):
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, batch in enumerate(train_loader):
            optimizer.zero_grad()
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
            predictions = torch.argmax(outputs.logits, dim=-1)
            correct += (predictions == labels).sum().item()
            total += labels.size(0)
            
            if batch_idx % 25 == 0:
                print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs}, –ë–∞—Ç—á {batch_idx}, Loss: {loss.item():.4f}")
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total
        print(f"‚úÖ –≠–ø–æ—Ö–∞ {epoch+1}: Loss={avg_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å={accuracy:.3f}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
                
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                predictions = torch.argmax(outputs.logits, dim=-1)
                
                val_correct += (predictions == labels).sum().item()
                val_total += labels.size(0)
        
        val_accuracy = val_correct / val_total
        print(f"üìä –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {val_accuracy:.3f}")
        model.train()
    
    # –†—É—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    save_dir = "./religious_detector_manual"
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª—å –≤ {save_dir}...")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ state_dict –º–æ–¥–µ–ª–∏
    torch.save(model.state_dict(), f"{save_dir}/pytorch_model.bin")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    model.config.save_pretrained(save_dir)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    tokenizer.save_pretrained(save_dir)
    
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
    loader_code = '''#!/usr/bin/env python3
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

class ReligiousDetector:
    def __init__(self, model_path="./religious_detector_manual"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.eval()
    
    def predict(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)
            prediction = torch.argmax(outputs.logits, dim=-1).item()
        
        return prediction, probs[0].numpy()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    detector = ReligiousDetector()
    
    examples = [
        "–†–µ–ª–∏–≥–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –ª—é–¥—è–º –Ω–∞–π—Ç–∏ —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏",
        "–í—Å–µ –≤–µ—Ä—É—é—â–∏–µ - –¥—É—Ä–∞–∫–∏ –∏ —Ñ–∞–Ω–∞—Ç–∏–∫–∏",
        "–î—É—Ö–æ–≤–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º–∏",
        "–¶–µ—Ä–∫–≤–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–∂–∂–µ–Ω—ã –¥–æ—Ç–ª–∞"
    ]
    
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
    for text in examples:
        pred, prob = detector.predict(text)
        class_name = "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π" if pred == 0 else "–û–ø–∞—Å–Ω—ã–π"
        confidence = prob[pred]
        
        print(f"\\n–¢–µ–∫—Å—Ç: {text}")
        print(f"–ö–ª–∞—Å—Å: {class_name}")
        print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}")
'''
    
    with open("load_trained_model.py", "w") as f:
        f.write(loader_code)
    
    print("üìù –°–æ–∑–¥–∞–Ω load_trained_model.py –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
    
    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
    print("\nüß™ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç...")
    model.eval()
    test_texts = ["–†–µ–ª–∏–≥–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –ª—é–¥—è–º", "–í—Å–µ –≤–µ—Ä—É—é—â–∏–µ –¥—É—Ä–∞–∫–∏"]
    
    for text in test_texts:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs.to(device))
            probs = torch.softmax(outputs.logits, dim=-1)
            pred = torch.argmax(outputs.logits, dim=-1).item()
        
        print(f"'{text}' -> {pred} ({'–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π' if pred == 0 else '–û–ø–∞—Å–Ω—ã–π'}) [{probs[0][pred]:.3f}]")
    
    print("üéâ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

if __name__ == "__main__":
    train_and_save_manually() 